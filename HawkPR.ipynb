{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STQz73i9y3Yr"
      },
      "source": [
        "Code for Hawks Process function tranlsated from matlab from chiangwe's HawkPR  repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTx8xNbm0fbe"
      },
      "source": [
        "## Python version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\dipiy\\anaconda3\\lib\\site-packages (1.24.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in c:\\users\\dipiy\\anaconda3\\lib\\site-packages (1.11.1)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\dipiy\\anaconda3\\lib\\site-packages (from scipy) (1.24.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q98MDzys1kwy"
      },
      "source": [
        "*Input variables*\n",
        "*(CSV Files)*\n",
        "\n",
        "Each has 2824 counties identified by an FIPS code\n",
        "1.   **Report** ;\n",
        "rows: 2824 x 6 (6 locations for cases);\n",
        "cols: 4 + 297 (dates) [15/02/20 - 07/12/20];\n",
        "records number of cases\n",
        "\n",
        "2. **Mobility**;\n",
        "rows: 2824 (counties);\n",
        "cols: 3 + 297 (dates);\n",
        "records amount of mobility\n",
        "\n",
        "3.   **Demography**;\n",
        "rows: 2824;\n",
        "cols: 9;\n",
        "records demographic identifiers of each county\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjxFVHwr0ebs",
        "metadata": {},
        "outputId": "0c54f8fe-6393-40eb-b75c-e5e8b44c740e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import time\n",
        "import scipy\n",
        "import random\n",
        "import scipy.stats as stats\n",
        "import scipy.sparse as sparse\n",
        "from scipy.stats import weibull_min, poisson\n",
        "from scipy.optimize import curve_fit\n",
        "from scipy.sparse import csc_matrix, eye\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.genmod.families import Poisson\n",
        "from sklearn.linear_model import PoissonRegressor\n",
        "from scipy import sparse\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU4hELFWQbwY"
      },
      "source": [
        "code uses a weibull distribution to model inter infection times. the parameters are updated within the code according to the expectation maximization algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def HawkPR(InputPath_report, InputPath_mobility, InputPath_demography, Delta, Alpha, Beta, EMitr, DaysPred, SimTimes, OutputPath_mdl, OutputPath_pred):\n",
        "    warnings.filterwarnings('ignore')\n",
        "\n",
        "    # Read in parameters\n",
        "    if Alpha == '' and Beta == '':\n",
        "        print('No shape and scale parameter for Weibull distribution provided. Use MLE to infer alpha and beta ... ')\n",
        "        alphaScale_in = 0\n",
        "        betaShape_in = 0\n",
        "    else:\n",
        "        alphaScale_in = float(Alpha)\n",
        "        betaShape_in = float(Beta)\n",
        "\n",
        "    if Delta == '':\n",
        "        print('No shift parameter for mobility provided. It will set to zero ... ')\n",
        "        mobiShift_in = 0\n",
        "    else:\n",
        "        mobiShift_in = int(Delta)\n",
        "\n",
        "    # Read-in COVID data\n",
        "    NYT = pd.read_csv(InputPath_report)\n",
        "\n",
        "    # Read-in mobility\n",
        "    Mobi = pd.read_csv(InputPath_mobility)\n",
        "\n",
        "    # Read-in demographic\n",
        "    Demo = pd.read_csv(InputPath_demography)\n",
        "    Demo_val = Demo.iloc[:, 3:].values\n",
        "\n",
        "    # Data pre-processing\n",
        "    covid = NYT.iloc[:, 3:].values\n",
        "    covid = NYT.iloc[:, 3:].apply(pd.to_numeric, errors='coerce').values  # Convert to numeric, coerce errors to NaN\n",
        "    covid[np.isnan(covid)] = 0\n",
        "    covid = np.hstack([np.zeros((covid.shape[0], 1)), np.diff(covid, axis=1)])\n",
        "    covid[covid <= 0] = 0\n",
        "\n",
        "    # Pad to shift\n",
        "    mob_head = Mobi.iloc[:, :4]\n",
        "    mob_val = Mobi.iloc[:, 4:].values\n",
        "\n",
        "    for _ in range(mobiShift_in):\n",
        "        mob_val = np.hstack([np.mean(mob_val[:, :7], axis=1, keepdims=True), mob_val])\n",
        "\n",
        "    # Get Key and Date\n",
        "    NYT_Date_list = NYT.columns[3:]\n",
        "    NYT_Key_list = NYT.iloc[:, :3].values\n",
        "\n",
        "    Mobi_Type_list = Mobi.iloc[:6, 3].values\n",
        "    Mobi_Date_list = Mobi.columns[4:]\n",
        "    Mobi_Key_list = Mobi.iloc[::6, :3].values\n",
        "\n",
        "    Demo_Type_list = Demo.columns[3:]\n",
        "    Demo_Key_list = Demo.iloc[:, 0].values\n",
        "\n",
        "\n",
        "    n_cty, n_day = covid.shape\n",
        "    n_mobitype = mob_val.shape[0] // n_cty\n",
        "\n",
        "    print(f'There are {n_cty} counties, {n_mobitype} types of Mobility indices, and {n_day} days in the COVID reports.')\n",
        "\n",
        "    # Train & Test Split\n",
        "    n_tr = covid.shape[1]\n",
        "    mob_tr = mob_val[:, :n_tr]\n",
        "    mob_te = mob_val[:, n_tr:n_tr+DaysPred]\n",
        "\n",
        "    # Normalization\n",
        "    mob_tr_reshape = mob_tr.reshape(n_mobitype,-1).T\n",
        "    mob_te_reshape = mob_te.reshape(n_mobitype, -1).T\n",
        "\n",
        "    Demo_val_in = Demo_val\n",
        "    Demo_val_tr = np.tile(Demo_val_in, (n_tr, 1))\n",
        "    Demo_val_te = np.tile(Demo_val_in, (DaysPred, 1))\n",
        "\n",
        "    covid_tr = covid\n",
        "\n",
        "    Covar_tr = np.hstack([mob_tr_reshape, Demo_val_tr])\n",
        "    Covar_te = np.hstack([mob_te_reshape, Demo_val_te])\n",
        "\n",
        "    Covar_tr_mean = np.mean(Covar_tr, axis=0)\n",
        "    Covar_tr_std = np.std(Covar_tr, axis=0)\n",
        "\n",
        "    Covar_tr = (Covar_tr - Covar_tr_mean) / Covar_tr_std\n",
        "    Covar_te = (Covar_te - Covar_tr_mean) / Covar_tr_std\n",
        "\n",
        "    # Get Variable names\n",
        "    #clean up variable names\n",
        "    VarNamesOld = np.concatenate([Mobi_Type_list, Demo_Type_list.T, ['Qprob']])\n",
        "    VarNames = [name.replace(' & ', '_').replace(' ', '_').lstrip('_') for name in VarNamesOld]\n",
        "\n",
        "    # Define Parameters\n",
        "    n_day_tr = n_day\n",
        "    T = n_day_tr\n",
        "    dry_correct = 2\n",
        "\n",
        "    emiter = EMitr\n",
        "    break_diff = 1e-3\n",
        "    day_for_tr = min(T - dry_correct, mob_tr.shape[1])\n",
        "\n",
        "    # Initialize Inferred Parameters\n",
        "    if (alphaScale_in == 0) and (betaShape_in == 0):\n",
        "        alpha = 2\n",
        "        beta = 2\n",
        "    else:\n",
        "        alpha = alphaScale_in\n",
        "        beta = betaShape_in\n",
        "\n",
        "    # Initial Weibull values\n",
        "    wbl_val = np.tile(np.tril(weibull_min.pdf(np.arange(1, n_day_tr+1)[:,None] - np.arange(1, n_day_tr+1), c=beta, loc=0, scale=alpha)), (n_cty, 1))\n",
        "\n",
        "    # K0 reproduction number, a function of time and mobility.\n",
        "    # K0 is a n_county * n_day by n_day matrix.\n",
        "    K0 = np.ones((n_cty, n_day_tr))\n",
        "    K0_ext_j = np.repeat(K0, n_day_tr, axis=0)\n",
        "\n",
        "    # q is a n_county * n_day by n_day matrix.\n",
        "    q = sparse.lil_matrix((n_cty * n_day_tr, n_day_tr))\n",
        "\n",
        "    # Mu is the background rate\n",
        "    mus = 0.5 * np.ones(n_cty)\n",
        "    mus = mus.reshape(n_cty , 1)\n",
        "\n",
        "    # lam is the event intensity\n",
        "    lam = np.zeros((n_cty, T))\n",
        "\n",
        "    '''\n",
        "    Debug lines\n",
        "    print(f'shape of wbl_val: {wbl_val.shape})')\n",
        "    print(f'shape of K0(reproductive number): {K0.shape})')\n",
        "    print(f'shape of K0_ext_j(adjusted): {K0_ext_j.shape})')\n",
        "    print(f'mus shape is: {mus.shape}')\n",
        "\n",
        "    '''\n",
        "\n",
        "    # EM iteration\n",
        "    alpha_delta = []\n",
        "    alpha_prev = []\n",
        "    beta_delta = []\n",
        "    beta_prev = []\n",
        "    mus_delta = []\n",
        "    mus_prev = []\n",
        "    K0_delta = []\n",
        "    K0_prev = []\n",
        "    theta_delta = []\n",
        "    theta_prev = []\n",
        "\n",
        "    for itr in range(emiter):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # E-step\n",
        "        '''\n",
        "        Expectation step\n",
        "        - given the parameters 0, alpha, beta and mus_c estimated from last iteration\n",
        "        - estimate latent variables p_c(i,j) for each county\n",
        "        - calculated using formula:\n",
        "\n",
        "        p_c(i,j) = R(x_c^t_j-delta, 0) w(t_i - t_j |alpha,beta) / lam_c(t_i)\n",
        "        {probability that injection i causes j}\n",
        "\n",
        "        p_c(i,i) = mus_c/ lam_c(t_i)\n",
        "        {probability that infection is imported}\n",
        "\n",
        "        '''\n",
        "        q = K0_ext_j * wbl_val * (covid_tr_ext_j(covid_tr, n_day_tr) > 0)\n",
        "\n",
        "        eye_mu = np.tile(np.identity(n_day_tr), (n_cty,1)) * np.tile(mus, (n_day_tr,1))\n",
        "        #eye_mu = np.kron(sparse.eye(n_day_tr), np.ones((n_cty, 1))) * np.tile(mus,(n_day_tr,1))\n",
        "        #eye_mu = eye_mu.toarray()\n",
        "        lam = np.sum(q * covid_tr_ext_j(covid_tr, n_day_tr) + eye_mu, axis=1)\n",
        "        lam = lam.reshape(lam.shape[0],1)\n",
        "        lam_eq_zero = lam == 0\n",
        "\n",
        "        q = np.divide(q,lam)\n",
        "        q[lam_eq_zero.flatten()] = 0\n",
        "\n",
        "        lam = lam.reshape(n_day_tr, n_cty).T\n",
        "        '''\n",
        "        print(f'shape of q: {q.shape}')\n",
        "        print(f'shape of covid_tr: {covid_tr_ext_j(covid_tr, n_day_tr).shape}')\n",
        "        print(f'shape of eye_mu: {eye_mu.shape}')\n",
        "        print(f'shape of lam: {lam.shape}')\n",
        "\n",
        "        '''\n",
        "\n",
        "        # M-step\n",
        "        '''\n",
        "        Maximisation step\n",
        "\n",
        "        maximise the data log likelihood wrt model parameters, done in three different opimisation problems\n",
        "        1) coefficient 0 from poisson regression\n",
        "        2) maximum likelihood estimation for shape and scale parameters (alpha and beta)\n",
        "        3) background rate mu determined analytically\n",
        "        '''\n",
        "        # Calculate Q, which stands for the average number (observed) of children generated by a SINGLE event j\n",
        "        # Note that the last \"dry_correct\" days of Q will be accurate, since we haven't observed their children yet\n",
        "        Q = np.reshape(q * covid_tr_ext_i(covid_tr, n_day_tr, n_cty), (n_day_tr, n_day_tr * n_cty))\n",
        "        Q = np.reshape(np.sum(Q, axis=0), (n_cty, n_day_tr))\n",
        "\n",
        "        # M-step Part 2)\n",
        "        # Weibull fitting\n",
        "        if (alphaScale_in == 0) and (betaShape_in == 0):\n",
        "            # Create the observation matrix\n",
        "            obs = np.tril(np.arange(1, day_for_tr + 1).reshape(-1, 1) - np.arange(1, day_for_tr + 1), -1).toarray()\n",
        "\n",
        "            # Calculate the frequency\n",
        "            freq = covid_tr_ext_j(covid_tr, n_day_tr) * covid_tr_ext_i(covid_tr, n_day_tr, n_cty) * q\n",
        "            freq = freq.toarray() if isinstance(freq, sparse.csr_matrix) else freq\n",
        "            freq = np.sum(np.transpose(freq.reshape(n_day_tr, n_cty, n_day_tr), (0, 2, 1)), axis=2)\n",
        "            freq = freq[:day_for_tr, :day_for_tr]\n",
        "\n",
        "            # Find indices where both obs and freq are positive\n",
        "            Ind_ret = np.where((obs > 0) & (freq > 0))\n",
        "            obs = obs[Ind_ret]\n",
        "            freq = freq[Ind_ret]\n",
        "\n",
        "            # Fit Weibull\n",
        "            def neg_log_likelihood(params):\n",
        "                alpha, beta = params\n",
        "                return -np.sum(freq * weibull_min.logpdf(obs, beta, scale=alpha))\n",
        "\n",
        "            # Initial guess for alpha and beta\n",
        "            initial_guess = [1.0, 1.0]\n",
        "\n",
        "            # Minimize the negative log likelihood\n",
        "            result = scipy.optimize.minimize(neg_log_likelihood, initial_guess, method='L-BFGS-B')\n",
        "            alpha, beta = result.x\n",
        "\n",
        "            # Create wbl_val\n",
        "            wbl_val = np.tile(np.tril(weibull_min.pdf(np.arange(1, n_day_tr + 1)[:,None] - np.arange(1, n_day_tr + 1), c=beta, scale=alpha)),(n_cty, 1))\n",
        "            #print(f'shape of wbl_val after fitting: {wbl_val.shape}')\n",
        "\n",
        "        # Estimate K0 and Coefficients for Poisson regression\n",
        "        glm_tr = np.nan_to_num(Covar_tr[:n_cty*day_for_tr, :])\n",
        "        glm_y = np.nan_to_num(Q[:, :day_for_tr].reshape(-1,1))\n",
        "        print(f'shape of glm_tr: {glm_tr.shape}')\n",
        "\n",
        "        glm_tr = sm.add_constant(glm_tr)\n",
        "\n",
        "        freqs = covid_tr[: , :day_for_tr].reshape(-1,1)\n",
        "        freqs = freqs.flatten()\n",
        "        np.nan_to_num(freqs)\n",
        "        print(f'shape of freqs: {freqs.shape}')\n",
        "\n",
        "        family = Poisson()\n",
        "        # Define the GLM model with cleaned data and specify missing='drop'\n",
        "        model = sm.GLM(glm_y, glm_tr, family=family, freq_weights=freqs, missing='drop')\n",
        "        print(model)\n",
        "\n",
        "        # Fit the model\n",
        "        result = model.fit(maxiter=300)\n",
        "        #print(result.summary())\n",
        "        '''\n",
        "        print(f' shape of result.params i.e. model coefficients = {(result.params).shape}')\n",
        "        print(f'shape of Covar_tr: {Covar_tr.shape}')\n",
        "        print(f'day_for_tr: {day_for_tr}')\n",
        "\n",
        "        '''\n",
        "\n",
        "        # M-step Part1)\n",
        "\n",
        "        ypred = result.predict(sm.add_constant(Covar_tr))\n",
        "\n",
        "        # Reshape ypred to match the original dimensions\n",
        "        K0 = np.reshape(ypred, (n_cty, n_day_tr))\n",
        "\n",
        "        # Bound K0\n",
        "        K0 = scipy.signal.savgol_filter(K0, window_length=5, polyorder=2)  # Adjust window_length and polyorder as needed\n",
        "        print(f'shape of bound K0: {K0.shape}')\n",
        "        K0_ext_j = np.repeat(K0, n_day_tr, axis=0)\n",
        "\n",
        "        # M-step Part 3)\n",
        "        # Estimate mu, the background rate\n",
        "\n",
        "        # Find where lam is zero\n",
        "        lam_eq_zero = np.where(lam[:, :day_for_tr] == 0)\n",
        "        # Estimate mu by dividing mus by lam\n",
        "        mus = mus / lam[:, :day_for_tr]\n",
        "\n",
        "        # Set mus to 0 where lam is 0\n",
        "        mus[lam_eq_zero] = 0\n",
        "        # Calculate the average of mus weighted by covid_tr\n",
        "        mus = np.sum(mus * covid_tr[:, :day_for_tr], axis=1) / day_for_tr\n",
        "        mus = mus.reshape(mus.shape[0],1)\n",
        "        #print(f'shape of mus (after M): {mus.shape}')\n",
        "        #print(mus)\n",
        "\n",
        "        # Convergence check\n",
        "        if itr == 0:\n",
        "            #save the first value\n",
        "            alpha_prev = alpha\n",
        "            beta_prev = beta\n",
        "            mus_prev = mus\n",
        "            K0_prev = K0\n",
        "            theta_prev = np.array(result.params)\n",
        "        else:\n",
        "            #calculate the RMSR\n",
        "            alpha_delta = np.hstack((alpha_delta, np.sqrt((alpha - alpha_prev)**2)))\n",
        "            beta_delta = np.hstack((beta_delta, np.sqrt((beta - beta_prev)**2)))\n",
        "            mus_delta = np.hstack((mus_delta, np.sqrt(np.mean((mus_prev - mus)**2))))\n",
        "            K0_delta = np.hstack((K0_delta, np.sqrt(np.mean((K0_prev - K0)**2))))\n",
        "            theta_delta = np.hstack((theta_delta, np.sqrt((np.sum((theta_prev - np.array(result.params))**2)) / len(np.array(result.params)))))\n",
        "\n",
        "            #save the current\n",
        "            alpha_prev = alpha\n",
        "            beta_prev = beta\n",
        "            mus_prev = mus\n",
        "            K0_prev = K0\n",
        "            theta_prev = np.array(result.params)\n",
        "            \n",
        "\n",
        "        # Early Stop\n",
        "        if itr > 5:\n",
        "          # Check the last 5 elements\n",
        "          rule = (np.all(alpha_delta[-5:] < break_diff) and \n",
        "                  np.all(beta_delta[-5:] < break_diff) and \n",
        "                  np.all(mus_delta[-5:] < break_diff) and \n",
        "                  np.all(K0_delta[-5:] < break_diff) and \n",
        "                  np.all(theta_delta[-5:] < break_diff))\n",
        "          \n",
        "          if rule:\n",
        "              print(\"Convergence Criterion Met. Breaking out of EM iteration...\")\n",
        "              break\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"ITR: Iteration {itr+1}, Elapsed time: {elapsed_time:.2f} seconds ------------------------------------------------\")\n",
        "\n",
        "    if itr == emiter - 1:\n",
        "        print('Reached maximum EM iteration.')\n",
        "\n",
        "\n",
        "    print(f\"---- E M Algorithm Completed -----------------------------------------------------------------------------------\")\n",
        "    # Start Simulation\n",
        "    np.savez(OutputPath_mdl, mus=mus, alpha=alpha, beta=beta, K0=K0, VarNames=VarNames, alpha_delta=alpha_delta, beta_delta=beta_delta, mus_delta=mus_delta, K0_delta=K0_delta, theta_delta=theta_delta)\n",
        "    loaded_data = np.load(OutputPath_mdl + '.npz')\n",
        "\n",
        "    # Get K0\n",
        "    print(f'shape of Covar_tr: {Covar_tr.shape}')\n",
        "    print(f'shape of Covar_te: {Covar_te.shape}')\n",
        "    Covar_all = np.vstack((Covar_tr, Covar_te))\n",
        "    n_day = n_day_tr + DaysPred\n",
        "    T_sim = n_day\n",
        "    Tlow = T_sim - DaysPred\n",
        "\n",
        "    # Predict\n",
        "    print(model.exog_names)\n",
        "    ypred = result.predict(sm.add_constant(Covar_all))\n",
        "    fK0 = ypred.reshape(n_cty, n_day)\n",
        "\n",
        "    # Make fK0 stable\n",
        "    fK0[fK0 > 4] = 4\n",
        "\n",
        "    # Simulation results\n",
        "    sim = np.zeros((n_cty, T_sim, SimTimes))\n",
        "\n",
        "    # Simulate offsprings\n",
        "    n_per_batch = 10**2\n",
        "    K0_sim = fK0[:, Tlow:]\n",
        "\n",
        "    for itr in range(SimTimes):\n",
        "        np.random.seed(itr)\n",
        "\n",
        "        # Calculate base rate\n",
        "        base = np.zeros((n_cty, DaysPred))\n",
        "        n_exh = np.zeros((n_cty, DaysPred))\n",
        "\n",
        "        t_stamps = np.arange(Tlow + 1, T_sim + 1)[:, None] - np.arange(1, Tlow + 1)  \n",
        "        intense = (np.tile(weibull_min.pdf(t_stamps, beta, scale=alpha), (n_cty, 1, 1)) * np.tile(fK0[:, :Tlow].reshape(n_cty, 1, Tlow), (1, DaysPred, 1)) *\n",
        "        np.tile(covid_tr[:, :Tlow].reshape(n_cty, 1, Tlow), (1, DaysPred, 1)))\n",
        "        \n",
        "        base = np.sum(intense, axis=2) + mus\n",
        "        n_exh = np.random.poisson(base)\n",
        "\n",
        "        for itr_cty in range(int(np.ceil(n_cty * 0.5))):\n",
        "            for itr_d in range(DaysPred):\n",
        "                max_d = DaysPred - itr_d\n",
        "\n",
        "                # Sample first\n",
        "                if n_exh[itr_cty, itr_d] > n_per_batch:\n",
        "                    n_batch = n_exh[itr_cty, itr_d] // n_per_batch\n",
        "                    cand = np.random.poisson(K0_sim[itr_cty, itr_d], size=n_per_batch)\n",
        "                    n_mod = n_exh[itr_cty, itr_d] % n_per_batch\n",
        "                    n_offs = np.sum(cand) * n_batch + np.sum(np.random.poisson(K0_sim[itr_cty, itr_d], size=n_mod))\n",
        "                else:\n",
        "                    n_offs = np.sum(np.random.poisson(K0_sim[itr_cty, itr_d], size=n_exh[itr_cty, itr_d]))\n",
        "\n",
        "                if n_offs > n_per_batch:\n",
        "                    n_batch = n_offs // n_per_batch\n",
        "                    n_mod = n_offs % n_per_batch\n",
        "\n",
        "                    sim_cand_wbl = np.ceil(weibull_min.rvs(alpha, scale=beta, size=n_per_batch))\n",
        "                    sim_cand_wbl = sim_cand_wbl[sim_cand_wbl <= max_d]\n",
        "                    sim_cand_wbl = np.histogram(sim_cand_wbl, bins=np.arange(1, max_d + 2))[0]\n",
        "\n",
        "                    t_delta = np.ceil(weibull_min.rvs(alpha, scale=beta, size=n_mod))\n",
        "                    t_delta = t_delta[t_delta <= max_d]\n",
        "                    nt = np.histogram(t_delta, bins=np.arange(1, max_d + 2))[0] + sim_cand_wbl * n_batch\n",
        "                else:\n",
        "                    t_delta = np.ceil(weibull_min.rvs(alpha, scale=beta, size=n_offs))\n",
        "                    t_delta = t_delta[t_delta <= max_d]\n",
        "                    nt = np.histogram(t_delta, bins=np.arange(1, max_d + 2))[0]\n",
        "\n",
        "                '''\n",
        "                Debugging\n",
        "                print(f'Outer loop: itr_cty : {itr_cty}')\n",
        "                print(f'Ineer loop: itr_day: {itr_d}')\n",
        "                print(f'shape of n_exh (which is number of offspring events: {n_exh.shape})')\n",
        "                print(f'First if-else block: {(n_exh[itr_cty, itr_d] > n_per_batch)}')\n",
        "                print(f'Second if-else block: {n_offs > n_per_batch}')\n",
        "                print(f'shape of nt: {nt.shape}')\n",
        "                n_exh[itr_cty, itr_d:] += nt\n",
        "                '''\n",
        "\n",
        "        sim[:, :, itr] = np.concatenate((covid_tr, n_exh), axis=1)\n",
        "\n",
        "    sim_out = sim[:, -DaysPred:, :]\n",
        "\n",
        "    # Format the output\n",
        "    sim_mean = np.mean(sim_out, axis=2)\n",
        "    print (sim_mean)\n",
        "    #Date_pred = pd.date_range(start=NYT_Date_list[-1], periods=DaysPred, freq='D').strftime('%Y_%m_%d').to_list()\n",
        "    #table_out = pd.DataFrame(sim_mean, columns=Date_pred)\n",
        "    #table_out = pd.concat([NYT.iloc[:, :3], table_out], axis=1)\n",
        "\n",
        "    #table_out.to_csv(OutputPath_pred, index=False)\n",
        "\n",
        "def covid_tr_ext_j(covid_tr, n_day_tr):\n",
        "    return np.repeat(covid_tr, n_day_tr, axis=0)\n",
        "\n",
        "def covid_tr_ext_i(covid_tr, n_day_tr, n_cty):\n",
        "    return np.tile(covid_tr.T, (1, n_day_tr)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KI0U-QcoH8vS",
        "metadata": {},
        "outputId": "81669b7d-059d-43c3-882e-3f9b6d48de5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   FIPS       State              County                Type  x2020-02-15  \\\n",
            "0  6037  California  Los_Angeles_County   _Grocery_pharmacy            0   \n",
            "1  6037  California  Los_Angeles_County              _Parks           13   \n",
            "2  6037  California  Los_Angeles_County        _Residential            0   \n",
            "3  6037  California  Los_Angeles_County  _Retail_recreation            1   \n",
            "4  6037  California  Los_Angeles_County   _Transit_stations           -1   \n",
            "\n",
            "   x2020-02-16  x2020-02-17  x2020-02-18  x2020-02-19  x2020-02-20  ...  \\\n",
            "0           -1            0            0           -1            0  ...   \n",
            "1           27           30           11           13           11  ...   \n",
            "2           -1            7            0            0           -1  ...   \n",
            "3            4            7           -1           -1            1  ...   \n",
            "4           -1          -11            2            1            2  ...   \n",
            "\n",
            "   x2020-02-22  x2020-02-23  x2020-02-24  x2020-02-24.1  x2020-02-25  \\\n",
            "0           -1            1           -1             -1            1   \n",
            "1          -13           13            8              8           13   \n",
            "2            1            0           -1             -1           -1   \n",
            "3           -1            4            1              1            2   \n",
            "4           -5            0            2              2            3   \n",
            "\n",
            "   x2020-02-26  x2020-02-27  x2020-02-28  x2020-02-29  x2020-03-01  \n",
            "0            2            2            4            4            3  \n",
            "1           11           14           10           11           -5  \n",
            "2           -1           -1           -1           -1            0  \n",
            "3            3            3            4            3            4  \n",
            "4            2            3            2            0           -3  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "Number of rows: 120\n",
            "Number of columns: 21\n"
          ]
        }
      ],
      "source": [
        "# Path to your CSV file in Google Drive\n",
        "InputPath_demography = r\"C:\\Users\\dipiy\\OneDrive\\Documents\\GitHub\\CausalSTPP\\TestData\\Demography_Test.csv\"\n",
        "InputPath_report = r\"C:\\Users\\dipiy\\OneDrive\\Documents\\GitHub\\CausalSTPP\\TestData\\Report_Test2.csv\"\n",
        "InputPath_mobility = r\"C:\\Users\\dipiy\\OneDrive\\Documents\\GitHub\\CausalSTPP\\TestData\\Mobility_Test.csv\"\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(InputPath_mobility)\n",
        "\n",
        "print(df.head())\n",
        "num_rows, num_columns = df.shape\n",
        "print(f'Number of rows: {num_rows}')\n",
        "print(f'Number of columns: {num_columns}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GkhREdRIP5WI",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "#scale of weibull\n",
        "Alpha = 4\n",
        "#shape of weibull\n",
        "Beta = 2\n",
        "\n",
        "# num of maximum iterations for EM algortihm in case convergence not reached\n",
        "EMitr = 20\n",
        "\n",
        "#additional days to be predicted by trained hawks process model\n",
        "DaysPred = 6\n",
        "\n",
        "#mobility shift parameter: ???\n",
        "Delta = 3\n",
        "\n",
        "SimTimes = 6\n",
        "\n",
        "#to_csv function will automatically create a csv file with this path\n",
        "OutputPath_pred = '/content/drive/My Drive/HawkPR_data_sim/Output.csv'\n",
        "OutputPath_mdl = r\"C:\\Users\\dipiy\\OneDrive\\Documents\\GitHub\\CausalSTPP\\TestData\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "DPr25HesaLKr",
        "metadata": {},
        "outputId": "e1c91649-f0ab-4c58-d7b0-18d63c81ebf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 20 counties, 6 types of Mobility indices, and 10 days in the COVID reports.\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A91EFDED0>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 1, Elapsed time: 0.07 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94944D50>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 2, Elapsed time: 0.02 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94B55950>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 3, Elapsed time: 0.02 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94B54110>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 4, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94944950>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 5, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94946B90>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 6, Elapsed time: 0.02 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A949450D0>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 7, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94945910>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 8, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94945890>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 9, Elapsed time: 0.02 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94B5A4D0>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 10, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94B58050>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 11, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94B5ABD0>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 12, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94B5B790>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 13, Elapsed time: 0.02 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94B5A4D0>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 14, Elapsed time: 0.02 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A9492F1D0>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 15, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A9492F850>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 16, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A9492F750>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 17, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94B25710>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 18, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94B25D50>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 19, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "shape of glm_tr: (160, 12)\n",
            "shape of freqs: (160,)\n",
            "<statsmodels.genmod.generalized_linear_model.GLM object at 0x0000025A94B244D0>\n",
            "shape of bound K0: (20, 10)\n",
            "ITR: Iteration 20, Elapsed time: 0.01 seconds ------------------------------------------------\n",
            "Reached maximum EM iteration.\n",
            "---- E M Algorithm Completed -----------------------------------------------------------------------------------\n",
            "shape of Covar_tr: (200, 12)\n",
            "shape of Covar_te: (120, 12)\n",
            "['const', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12']\n",
            "[[23.66666667 25.5        19.16666667 18.33333333 21.66666667 22.83333333]\n",
            " [17.33333333 21.5        21.83333333 21.5        18.5        22.66666667]\n",
            " [12.83333333 12.33333333 10.33333333 13.          9.16666667 10.83333333]\n",
            " [ 0.66666667  2.          1.          0.66666667  1.16666667  1.66666667]\n",
            " [ 1.83333333  2.33333333  3.33333333  3.5         2.33333333  2.33333333]\n",
            " [ 6.33333333  6.33333333  4.16666667  3.33333333  4.16666667  3.83333333]\n",
            " [10.83333333  9.83333333 11.83333333 10.16666667  8.5        10.16666667]\n",
            " [10.33333333 11.66666667  9.5        10.33333333  7.66666667  8.33333333]\n",
            " [ 0.16666667  0.16666667  0.83333333  1.5         0.33333333  0.        ]\n",
            " [ 0.66666667  1.83333333  1.5         1.16666667  0.66666667  1.        ]\n",
            " [ 1.66666667  0.83333333  1.          0.83333333  1.          0.83333333]\n",
            " [ 0.33333333  0.66666667  0.5         1.          0.5         0.5       ]\n",
            " [ 0.          0.16666667  0.          0.          0.          0.        ]\n",
            " [ 3.33333333  6.16666667  4.5         6.33333333  7.83333333  3.83333333]\n",
            " [ 9.          9.5         9.5        12.5        10.66666667  9.83333333]\n",
            " [ 1.5         3.5         4.33333333  2.33333333  3.          2.66666667]\n",
            " [ 3.          2.5         2.16666667  3.66666667  2.83333333  2.66666667]\n",
            " [ 6.16666667  6.          7.16666667  5.83333333  6.16666667  5.66666667]\n",
            " [36.33333333 46.66666667 50.5        43.66666667 33.66666667 30.83333333]\n",
            " [16.33333333 14.16666667 18.16666667 21.33333333 14.         16.5       ]]\n"
          ]
        }
      ],
      "source": [
        "HawkPR(InputPath_report, InputPath_mobility, InputPath_demography, Delta, Alpha, Beta, EMitr, DaysPred, SimTimes, OutputPath_mdl, OutputPath_pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tmOlloGI0bGZ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
